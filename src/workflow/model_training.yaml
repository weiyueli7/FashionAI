# PIPELINE DEFINITION
# Name: model-training-pipeline
components:
  comp-model-training:
    executorLabel: exec-model-training
deploymentSpec:
  executors:
    exec-model-training:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_training
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.10.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_training():\n    print(\"Model Training Job\")\n\n    import\
          \ google.cloud.aiplatform as aip\n\n    # Hardcoded values based on the\
          \ shell script\n    project = \"fashion-ai-438801\"  # Replace with your\
          \ GCP project ID\n    location = \"us-central1\"  # Replace with your GCP\
          \ region\n    staging_bucket = \"gs://fashionai_training\"  # Replace with\
          \ your staging bucket URI\n    gcs_data_bucket_uri = \"gs://fashionai_training\"\
          \  # Replace with your GCS data bucket URI\n\n    # Initialize Vertex AI\
          \ SDK for Python\n    aip.init(project=project, location=location, staging_bucket=staging_bucket)\n\
          \n    # Prebuilt container image and Python package details\n    container_uri\
          \ = \"us-docker.pkg.dev/vertex-ai/training/pytorch-xla.2-3.py310:latest\"\
          \n    python_package_gcs_uri = f\"{staging_bucket}/trainer.tar.gz\"\n  \
          \  python_module_name = \"trainer.task\"\n\n    # Arguments passed to the\
          \ training script\n    cmdargs = [\n        f\"--output_dir={gcs_data_bucket_uri}/test_fclip/\"\
          ,\n        \"--n_samples=100\",\n        \"--bucket_name=fashionai_training\"\
          \n    ]\n\n    # Machine configuration\n    machine_type = \"n1-standard-4\"\
          \n    replica_count = 1\n\n    # Display name for the custom training job\n\
          \    display_name = \"fashionclip_training_job\"\n\n    print(python_package_gcs_uri,\
          \ \"python_package_gcs_uri\")\n\n    # Create the CustomPythonPackageTrainingJob\
          \ object\n    job = aip.CustomPythonPackageTrainingJob(\n        display_name=display_name,\n\
          \        python_package_gcs_uri=python_package_gcs_uri,\n        python_module_name=python_module_name,\n\
          \        container_uri=container_uri,\n        project=project,\n    )\n\
          \n    # Run the training job\n    job.run(\n        args=cmdargs,\n    \
          \    replica_count=replica_count,\n        machine_type=machine_type,\n\
          \        #base_output_dir=f\"{gcs_data_bucket_uri}\",\n        sync=True,\
          \  # Wait for the job to finish\n    )\n\n    print(f\"Training job '{display_name}'\
          \ submitted successfully.\")\n\n"
        image: python:3.10
pipelineInfo:
  name: model-training-pipeline
root:
  dag:
    tasks:
      model-training:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-training
        taskInfo:
          name: model-training
schemaVersion: 2.1.0
sdkVersion: kfp-2.10.1
